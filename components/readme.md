# 微博事件分析流水线

本项目是一个用于处理和分析微博舆情事件的自动化数据处理流水线。脚本接收包含相关微博内容的 CSV 文件作为输入，通过多阶段处理，最终输出事件综合分析报告，涵盖事件严重程度、影响范围、话题倾向和敏感度等关键指标。


## ✨ 功能特性
- **自动化流水线**：从数据预处理、模型推理到分析整合，实现端到端自动化，无需人工干预中间环节。
- **混合模型策略**：结合「本地微调模型」（用于立场检测，兼顾效率与定制化）与「云端 API 模型」（用于事件敏感度分析，保障分析深度），平衡性能与分析质量。
- **多维度分析**：从多角度评估事件属性，覆盖：
  - 话题倾向（支持 / 反对 / 中立）
  - 敏感程度（高 / 中 / 低）
  - 事件热度（分时段统计，如近1小时、24小时）
  - 紧急程度（需响应优先级）
  - 影响范围（传播圈层、地域覆盖等）
- **结果持久化**：所有中间过程数据（如模型输入、推理结果）和最终分析报告均保存至 `output` 文件夹，便于追溯数据流转和调试问题。
- **报告汇总**：每次运行结果自动追加至 `final_summary_report.csv`，支持多事件统一管理、对比分析，适用于批量舆情监测场景。


## 📂 项目结构
```bash
.
├── components/         # 核心处理模块（数据转换、模型调用、指标计算等）
│   ├── __init__.py
│   ├── stance.py       # 话题立场检测逻辑（调用本地微调模型）
│   ├── sensitivity.py  # 事件敏感度分析逻辑（调用云端API模型）
│   └── ...             # 其他辅助模块（如数据清洗、指标计算）
├── output/             # 输出文件存放目录（脚本运行时自动创建）
├── main.py             # 主执行脚本：调度整个流水线的执行顺序
└── README.md           # 项目说明文档（环境配置、运行指南等）
```


## ⚙️ 环境准备与配置
运行脚本前，需完成以下环境搭建和参数配置步骤。

### 1. 安装依赖
项目依赖第三方 Python 库，建议先创建虚拟环境（如 `venv` 或 `conda`），再通过 `requirements.txt` 安装依赖。

#### 步骤1：创建 `requirements.txt`
根据 `components` 模块的实现需求，示例依赖如下（需根据实际代码补充调整）：
```txt
# requirements.txt 示例内容
transformers>=4.30.0    # 本地微调模型加载与推理
torch>=2.0.0            # 深度学习框架（支持GPU加速）
pandas>=2.0.0          # CSV文件读写与数据预处理
requests>=2.31.0        # 云端API调用
tqdm>=4.65.0            # 进度条显示
```

#### 步骤2：安装依赖
在虚拟环境中执行以下命令：
```bash
pip install -r requirements.txt
```


### 2. 配置脚本参数
打开 `main.py` 文件，找到「全局配置（Global Configuration）」部分，根据自身环境修改以下关键变量：

#### （1）输入文件路径
指定待处理的微博数据 CSV 文件（需确保 CSV 包含「微博内容」「发布时间」等核心字段）：
```python
# main.py 全局配置 - 输入文件
CSV_INPUT_FILE = '/path/to/your/input_file.csv'  # 替换为你的CSV文件绝对路径
```

#### （2）本地微调模型路径
指定用于立场检测的本地微调模型文件夹（需确保模型格式符合 `transformers` 加载要求）：
```python
# main.py 全局配置 - 本地模型
FINETUNED_MODEL_PATH = "/path/to/your/finetuned_model"  # 替换为你的微调模型路径
```

#### （3）云端 API 模型配置
填写云端 API 服务商的密钥、接口地址和模型名称（如阿里云 DashScope、OpenAI 等）：
```python
# main.py 全局配置 - 云端API
API_KEY = "sk-your_api_key_here"                  # 替换为你的API密钥
API_BASE_URL = "https://your_api_provider.com/v1" # 替换为API接口基础地址
API_MODEL_NAME = "qwen-plus"                      # 替换为你使用的API模型名（如gpt-3.5-turbo）
```

#### （4）Few-shot 示例文件路径
指定用于云端 API 模型「少样本学习」的示例文件（帮助模型理解任务格式，提升分析准确性）：
```python
# main.py 全局配置 - Few-shot示例
FEW_SHOT_EXAMPLES_PATH = "/path/to/your/few_shot_examples.csv"  # 替换为示例文件路径
```


## 🚀 如何运行
完成环境配置后，在终端中执行以下命令启动流水线：
```bash
python main.py
```

脚本会按以下阶段顺序执行，并在控制台打印实时进度：

1. **阶段 1：数据预处理**  
   清洗 CSV 中的无效数据（如空内容、重复微博），转换数据格式，生成模型可输入的结构化数据。

2. **阶段 2：模型推理**  
   - 调用本地微调模型，输出每条微博的「话题立场」结果；
   - 调用云端 API 模型，输出事件的「敏感程度」评分；
   （此阶段耗时取决于数据量，建议耐心等待）

3. **阶段 3：分析与整合**  
   基于模型推理结果，计算事件热度（分时段统计）、紧急程度（结合敏感度与传播速度）、影响范围（基于发布者地域、粉丝量）等指标。

4. **阶段 4：结果汇总与 CSV 输出**  
   将所有分析结果整理为文本报告和 CSV 汇总表，保存至 `output` 目录。


## 📊 输出文件说明
所有输出文件均存储在 `output/` 目录下，各文件用途如下：

| 文件名                  | 说明                                                                 |
|-------------------------|----------------------------------------------------------------------|
| **stance_tendency.jsonl**  | 立场检测模型的输入数据（JSON Lines 格式，每条记录含微博ID、内容等）   |
| **event_sensitivity.jsonl** | 敏感度分析模型的输入数据（JSON Lines 格式，含预处理后的事件相关文本） |
| **content_from_finetune.jsonl** | 本地微调模型的推理结果（含每条微博的立场标签、置信度）               |
| **content_from_api.jsonl**     | 云端 API 模型的推理结果（含事件敏感度评分、关键敏感词）               |
| **final_event_severity.txt**   | 事件严重程度的详细文本报告（含严重等级、判定依据）                   |
| **final_event_impact.txt**     | 事件影响范围的详细文本报告（含传播圈层、地域覆盖、涉及人群）         |
| **final_summary_report.csv**   | 核心汇总报告（每次运行追加一行），含事件ID、分析时间、所有关键指标，支持多事件对比 |